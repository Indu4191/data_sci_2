{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Support Vector Machines (SVM)\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand how the SVM builds its decision threshold.\n",
    "- Understand the concept of the maximum margin hyperplane.\n",
    "- Visualize the linearly separable case in classification.\n",
    "- Understand the math behind finding the maximum margin hyperplane.\n",
    "- Understand the hinge loss for SVM.\n",
    "- Understand how the regularization constant C allows SVMs to fit non-linearly separable problems.\n",
    "- See how the kernel trick transforms problems from non-linearly separable to linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to SVMs](#intro)\n",
    "- [How does the SVM classifiy?](#how-classify)\n",
    "- [Intuition behind the decision boundary](#intuition)\n",
    "- [Why maximize the margin?](#why-max-margin)\n",
    "- [SVM origins: the perceptron algorithm](#perceptron)\n",
    "- [The maximum margin hyperplane](#mmh)\n",
    "    - [Finding the maximum margin](#finding-mmh)\n",
    "- [The hinge loss](#hinge)\n",
    "    - [\"Slack\"](#slack)\n",
    "    - [The regularizing hyperparameter C](#hyper-c)\n",
    "- [The kernel trick](#kernel)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to SVMs\n",
    "\n",
    "---\n",
    "\n",
    "The Support Vector Machine (SVM) algorithm is a different approach to classification.\n",
    "\n",
    "SVM still fits a decision boundary like a logistic regression regression, but uses a different loss function called the \"hinge loss\" (as opposed to the log loss in logistic regression).\n",
    "\n",
    "This lesson will overview the details of the SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='how-classify'></a>\n",
    "\n",
    "## How does the SVM classify?\n",
    "\n",
    "---\n",
    "\n",
    "It's important to start with the intuition for SVM with the special _**linearly separable**_ classification case.\n",
    "\n",
    "If classification of observations is \"linearly separable\", SVM fits the **\"decision boundary\"** that is defined by the largest margin between the closest points for each class. This is commonly called the **\"maximum margin hyperplane (MMH)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM](./assets/linear_separability_vs_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='intuition'></a>\n",
    "\n",
    "## Intuition behind the SVM decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "SVM's criterion for a decision surface is one that is _maximally far away from any data point between classes_. The distance from the decision boundary to the closest data point determines the \"margin\" of the classifier.\n",
    "\n",
    "The points SVM uses to fit the decision boundary are called \"support vectors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/Margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='why-max-margin'></a>\n",
    "\n",
    "## Why maximize the margin?\n",
    "\n",
    "---\n",
    "\n",
    "**SVM solves for a decision boundary that should theoretically minimize the generalization error.** \n",
    "\n",
    "Observations that are near the decision boundary between the classes are the observations with the most \"ambiguous\" labels. They are the observations that are approaching equal probability to be one class or the other (given the predictors).\n",
    "\n",
    "SVM, instead of considering all the observations \"equally\" in the loss function it minimizes, defines it's fit using the most ambiguous points. It's decision boundary is safe in that errors in new measured observations are not likely to cause the SVM to mis-classify.\n",
    "\n",
    "The SVM is concerned with generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='perceptron'></a>\n",
    "\n",
    "## SVM origins: the perceptron algorithm\n",
    "\n",
    "---\n",
    "\n",
    "The perceptron algorithm is a linear function of weights $w$ and predictors $X$ that assigns points to binary classes. If the function returns a value greater than 0 then the observation is classified as 1, otherwise it is classified as zero.\n",
    "\n",
    "$f(X)$ below is the perceptron function to determine the classes. $b$ here is known as the \"bias\", which is analagous to the intercept term.\n",
    "\n",
    "### $$ f(X) = \\begin{cases}1 & \\text{if }w \\cdot x + b > 0\\\\0 & \\text{otherwise}\\end{cases} $$\n",
    "\n",
    "If the points are linearly seperable, the solving the perceptron is guaranteed to converge on a solution, but that solution is not necessarily optimal for future observations. This led to the invention of the SVM, which finds the optimal discriminator: the maximum margin hyperplane.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mmh'></a>\n",
    "\n",
    "## The maximum margin hyperplane\n",
    "\n",
    "---\n",
    "\n",
    "We choose a normalizing constant such that the distance from the plane to the closest points of either classes will be 1.\n",
    "\n",
    "### $$ w^T x_{+} + b = 1 \\\\ w^T x_{-} + b = -1 $$\n",
    "\n",
    "For the distance to the closest positive and negative class points, respectively (known as \"support vectors\").\n",
    "\n",
    "If the normalizer for the weights is $||w||$, the size of the margin is then:\n",
    "\n",
    "### $$ \\text{margin} = \\frac{2}{||w||} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/linear_sep_support_vecs_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='finding-mmh'></a>\n",
    "### Finding the maximum margin\n",
    "\n",
    "We want to find a distance between these points that is the widest possible. Therefore, we are looking to maximize the value $\\frac{2}{||w||}$.\n",
    "\n",
    "So, maximize the weights with constraints on what the function can output. When the target is 1, the function needs to be 1 or greater. When the target is 0 (or -1), the function needs to output -1 or lower.\n",
    "\n",
    "### $$ \\underset{w}{\\text{max}} \\frac{2}{||w||} \\quad \\text{subject to} \\begin{cases}\\text{if } y_i = 1 & w^T x_i + b \\ge 1 \\\\ \\text{if } y_i = -1 & w^T x_i + b \\le -1 \\end{cases} \\text{for } i \\text{ in } N$$\n",
    "\n",
    "Note here that $y$ is specified as either 1 or -1, as opposed to the 0, 1 that we are used to. This is convenient for converting this to be a minimization. To make this a minimization, we can change the equation to be:\n",
    "\n",
    "### $$ \\underset{w}{\\text{min }} ||w||^2 \\quad \\text{subject to} \\quad y(w^T x_i + b) \\ge 1 $$\n",
    "\n",
    "Which works out because when $y = -1$ the values become positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/linear_sep_support_vecs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hinge'></a>\n",
    "\n",
    "## The hinge loss and non-linearly separable cases\n",
    "\n",
    "---\n",
    "\n",
    "In cases where there is no line or plane that can separate all of the points perfectly, we need to introduce the capacity for model error. Using the constraint above that $y(w^T X + b) \\ge 1$, we can introduce the **hinge loss function**:\n",
    "\n",
    "### $$ \\text{hinge loss} = \\sum_{i=1}^n \\max\\left(0, 1 - y_i(w^T x_i + b)\\right) $$\n",
    "\n",
    "Where now, if the point is on the correct side (correctly classified), the value will be 0. If the point is not $\\ge 1$, this function will be greater than zero.\n",
    "\n",
    "Using $f(x_i) = (w^T x_i + b)$, \n",
    "\n",
    "### $$\\text{hinge loss} = \\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right)$$ \n",
    "\n",
    "If $f(x_i) > 1$, the point lies _outside_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) = 1$ the point is _on_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) < 1$ the point lies _inside_ the margin and **does** contribute to the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Hinge loss](./assets/hinge_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='slack'></a>\n",
    "\n",
    "### Hinge loss and \"slack\"\n",
    "\n",
    "When we have a scenario where it is not possible to perfectly separate, we use the hinge loss with a regularization constant $C$:\n",
    "\n",
    "### $$ \\min ||w||^2 + C\\sum_{i=1}^N \\epsilon_i \\quad \\text{subject to} \\quad y(w^T x_i + b) \\ge 1 - \\epsilon_i $$\n",
    "\n",
    "Where the $\\epsilon$ are the errors of the classifier, and the $C$ is a regularization term that determines how much the classification errors matter (relative to the maximization of the margin).\n",
    "\n",
    "The function that the SVM minimizes to find the boundary is:\n",
    "\n",
    "### $$  \\underset{w}{\\text{min }} ||w||^2 + C\\sum_{i=1}^N max\\left(0, 1 - y_i(w^T x_i + b)\\right) $$\n",
    "\n",
    "A small $C$ creates a wider margin because errors will matter less. A large $C$ creates a tighter margin because errors matter more. An infinite $C$ parameter is a \"hard\" margin, which always minimizes error over the size of the boundary.\n",
    "\n",
    "We are trying to minimize the norm of the weights $||w||$ and thus maximize the margin, but now we are also trying to minimize our errors at the same time. We have a balance in our minimization between how wide the margin should be and how much error we tolerate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hyper-c'></a>\n",
    "### The regularizing hyper-parameter $C$\n",
    "\n",
    "By setting the hyper-parameter $C$ we can control the extent to which our SVM is tolerant to misclassification errors. It is sometimes called the \"soft-margin constant\". \n",
    "\n",
    "$C$ affects the strength of the \"penalty\", similar to the lambda terms in the Ridge and Lasso. \n",
    "\n",
    "By multiplying the sum of the errors, which are the distances from the margins to the points inside of the margin, it allows the SVM to classify non-linearly separable problems by allowing errors to occur. \n",
    "\n",
    "The lower the value of $C$, the more misclassified observations errors are allowed. These misclassified points are known as \"slack variables\". Reducing the effect of errors on the loss function puts the emphasis on widening the margin.\n",
    "\n",
    "For those interested in exporing the math more, [there is a good tutorial here.](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/#more-457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./assets/slack_variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./assets/soft_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kernel'></a>\n",
    "## The \"kernel trick\" for non-linearly separable problems\n",
    "\n",
    "---\n",
    "\n",
    "The \"kernel trick\" allows an SVM to classify non-linearly separable problems. It is a big reason why SVMs are so popular.\n",
    "\n",
    "The idea behind the kernel trick is that you can arbitrarily transform your observations that _have no linear separability_ by putting them into a different \"dimensional space\" where the DO have linear separability, fit an SVM in that higher dimensional space, and then invert the transformation of the data and the model itself back into the original space.\n",
    "\n",
    "This is done by \"wrapping\" your predictors in a kernel function that transforms them into this higher dimensional space. \n",
    "\n",
    "[Check out these lecture slides for more detail.](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "The following pictures should give you a general intuition for what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./assets/kernel_trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./assets/kernel_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![polynomial kernel](./assets/nonlinear-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![gaussian kernel](./assets/nonlinear-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- [For a really great resource check out these slides (some of which are cannabalized in this lecture).](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "- [This website is also a great resource, on a slightly more technical level.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)\n",
    "- SVM docs on [SKLearn](http://scikit-learn.org/stable/modules/svm.html)\n",
    "- Iris example on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#example-svm-plot-iris-py)\n",
    "- Hyperplane walkthrough on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py)\n",
    "- A comprehensive [user guide](http://pyml.sourceforge.net/doc/howto.pdf) to SVM. My fav!\n",
    "- A [blog post tutorial](http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/) of understanding the linear algebra behind SVM hyperplanes. Check [part 3](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/) of this blog on finding the optimal hyperplane\n",
    "- This [Quora discussion](https://www.quora.com/How-do-you-teach-Support-Vector-Machine-to-a-beginner-in-Machine-Learning) includes a high-level overview plus a [20min video](https://www.youtube.com/watch?v=aDbsJ_S3tIA) walking through the core \"need-to-knows\"\n",
    "- A [slideshow introduction](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf) to the optimization considerations of SVM\n",
    "- A second [slideshow overview from UCF](http://www.cs.ucf.edu/courses/cap6412/fall2009/papers/Berwick2003.pdf) on the highnotes of SVM\n",
    "- Andrew Ng's [notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) on SVM from CS 229\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=eHsErlPJWUU) (1hr+) from one of my fav lecturers (Dr Yasser) on SVM. He does a followup on [kernel tricks](https://www.youtube.com/watch?v=XUj5JbQihlU) too\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=_PwhiWxHK8o) (50min) (from MIT Opencoursewar)\n",
    "- An infamous [paper](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf) (cited 7000+ times!) on why SVM is a great text classifier\n",
    "- An [advanced discussion](http://www.icml-2011.org/papers/386_icmlpaper.pdf) of SVMs as probabilistic models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
